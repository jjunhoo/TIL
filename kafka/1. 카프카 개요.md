## 1.1 카프카 개요

### 잘란도와 트위터의 카프카 도입 사례

- 카프카를 왜 사용해야 하는지 또는 카프카를 도입하는 것이 맞는지에 대한 의구심과 같은 의문점을 '잘란도'와 '트위터' 기업의 카프카 사용 사례를 통해 카프카 도입을 통한 장점 분석


### 유럽 최대 온라인 패션몰 잘란도의 도전 사례

- 신발 판매에 주력하던 잘란도는 사업 확장을 위해 2010년 네덜란드와 프랑스에 자사 서비스를 출시하며 의류 판매 사업 추가 (국내 '무신사'와 유사)
- 잘란도는 유럽 최대의 패션 플랫폼으로 성장하며 2020년 기준, 실사용자 수 3,100만 명, 연간 주문수 1억 4,500만 건, 상품 판매 건수는 50만 건 기록
- 점차 회사의 규모가 커지고, 사업도 다각화 되며, 데이터에 대한 요구사항이 다양해짐
    - 이러한 요구사항의 근본적인 해결책으로 이벤트 드리븐 시스템 전환 결정
    - 데이터를 소비하는 컨슈머들은 자신의 요구사항에 따라 데이터를 처리하거나 구독할 수 있게 됨

> 위기에 봉착한 잘란도

- 이벤트 드리븐 시스템 아키텍처를 도입하기로 했지만, 어떻게 사용하는 것이 효율적일까에 대한 고민이 필요했음
- 이벤트 드리븐 시스템 구성에서 가장 중요한 사항
    - 인바운드 데이터와 아웃바운드 데이터의 동일성
    - 통신 방법
    - 데이터 정합성
    - 이벤트 순서 보장

> 비동기 방식의 대표 스트리밍 플랫폼, 카프카 도입

- 동기 방식의 한계점을 느끼고 비동기 방식으로 전환을 고려한 잘란도는 카프카 도입 결정
- 카프카 기능의 장점
    - 높은 처리량
    - 순서 보장
    - 적어도 한 번(At least once) 전송 방식
    - 파티셔닝
    - 백프레셔 핸들링
    - 로그 컴팩션

> 빠른 데이터 수집이 가능한 높은 처리량

- HTTP 기반으로 전달되는 이벤트일지라도, 이벤트가 카프카로 처리되는 응답시간은 불과 밀리초(ms) 단위

> 순서 보장

- Entity 간의 유효성 검사나 동시 수정 같은 무수한 복잡성들이 제거됨으로써 구조가 간결해짐

> 적어도 한 번 (At least once) 전송 방식

- 분산된 여러 네트워크 환경에서의 데이터 처리에서 중요한 부분은 '멱등성'
    - 동일한 작업을 여러 번 수행하더라도 결과가 달라지지 않는 것을 의미
    - producer가 재전송을 하더라도 데이터 변화 없음
- '적어도 한 번 (At least once) 방식 사용 시 간혹 이벤트 중복이 발생할 수는 있으나, 누락 없이 재전송 가능
    - 메시지 손실 X
- 백엔드 시스템의 중복 메시지 처리 가능 시 복잡한 트랜잭션 처리 불필요

> 백프레셔 핸들링

- 카프카 클라이언트는 Pull 방식으로 동작
- Pull 방식의 장점
    - 자신의 속도로 데이터 처리 가능
- Push 방식의 한계
    - 브로커가 보내주는 속도에 의존

> 파티셔닝

- 논리적으로 토픽을 여러 개로 나눌 수 있음
- 각 파티션들을 다른 파티션들과 관계없이 처리 가능하므로 효과적인 수평 확장 가능

> 그 외 여러 가지 기능

- 로그 컴팩션 기능을 통한 스냅샷 역할 가능
- producer와 consumer의 분리된 비동기식 방식 사용에 따른 어플리케이션의 병목 현상 파악 가능

> 카프카로 도약의 기회를 얻은 잘란도

- 잘란도는 이벤트 스트림을 통해 모든 데이터를 비동기 방식으로 처리
- 또한 성장하고 있는 팀이나 조직 등에서 카프카를 쉽게 운영하거나 사용할 수 있으며 적절한 확장도 용이하도록 구성
- 내부 데이터 처리를 간소화하고, 높은 처리량을 바탕으로 스트림 데이터 처리의 확장성 또한 높임

### SNS 절대 강자 트위터의 카프카 활용 사례

- 사용자가 글을 게시하면, 그 사용자를 팔로우하는 다른 사용자들에게 메시지가 즉시 전달
- 실시간 서비스인 트위터에서의 실시간 요구사항들로 인하여 데이터 엔지니어링 팀은 아래와 같은 문제 봉착
    - 빠른 속도 서빙
    - 많은 실시간 사용 사례 수집
- 이러한 워크로드를 처리할 수 있는 시스템으로 이벤트 버스를 구축해 운영했지만, 카프카로 전환

### 카프카로 유턴한 트위터

- 트위터가 처음 카프카 0.7 버전을 사용할 당시, 카프카는 I/O 오퍼레이션 문제, 내구성, 레플리케이션 미구현 등의 불안정성 이슈가 있었음
- 따라서 트위터는 카프카 대신 인하우스 메시지 시스템 구축
- 이후 빠른 응답시간, 높은 처리량, Pub/Sub 모델 지원 등의 카프카의 성장으로 트위터는 다시 카프카 재검토

> 비용 절감 효과

- 카프카 재도입 결정 전, 트위터에서는 이벤트 버스에서 실행되는 워크로드 (안정적인 쓰기, 마지막 읽기, 따라잡기 읽기)와 일부 장애 시나리오를 통해 카프카 성능 테스트 진행
  - 성능 측면에서 메시지 처리 양과 관계없이 카프카의 응답 속도가 더 빠르다는 사실 입증
  - 이벤트 버스 
    - 서빙 레이어와 스토리지 레이어의 분리로 인한 추가 홉 필요
    - fsync() 를 하는 동안 블로킹
  - 카프카
    - 하나의 프로세스에서 스토리지와 요청을 모두 처리
    - fsync() 를 처리하고 제로 카피 (zero-copy : Network에서 Read/Write 할때 걸리는 불필요한 Copy 과정을 최소화) 사용

> 강력한 커뮤니티

- 카프카를 향상하고, 버그를 수정하며 새로운 기능을 개발하고 기여하는 엔지니어들이 전 세계에 수백 명
- 오픈 소스 프로젝트로 유명하여 다양한 사용자 경험을 인터넷을 통하여 습득 가능
- 카프카의 대중화로 인하여 기업에서 데이터 엔지니어를 고용하기 쉬움

## 1.2 국내외 카프카 이용 현황

### 해외 카프카 이용 현황

- 컨플루언트는 카프카 프로젝트 리딩
  - 카프카 관련 내용을 자사 블로그와 발표 자료 등을 통해 공개
- 2020년 기준, 포춘 100대 기업 중 80% 이상이 카프카 사용

- 라인 (LINE) 내부의 50여 개 서비스들이 카프카 이용 
  - 하루 2,500억 건 이상 메시지 처리
  - 210TB의 데이터가 카프카로 인입
- 뉴욕타임즈 
  - 카프카와 카프카 스트림즈 API를 이용하여 실시간으로 콘텐츠 배포
- 아이다스
  - 카프카를 이용하여 데이터 소스 시스템 통합 및 모니터링, 분석 작업 실시간 처리
- 데이터독 (Datadog)
  - 각종 메트릭 및 이벤트 통합 파이프라인으로 카프카 사용
- 스포티파이 (Spotify)
  - 로그 전송 시스템으로 카프카 사용

### 국내 카프카 이용 현황

- IT, 쇼핑, 배송, 통신 등 많은 기업에서 카프카를 검토하거나 이미 도입하여 운영
- 2018년 당시 카카오는 총 7개의 클러스터 보유
  - 하루 2,600억 건의 메시지 처리 
  - 하루 약 240TB 데이터가 카프카로 인입

## 1.3. 카프카의 주요 특징

> 높은 처리량과 낮은 지연시간

![캡처](https://cdn.confluent.io/wp-content/uploads/throughput-and-latency-quantiles.png)

- 카프카, Pulsar, RabbitMQ 성능 비교표를 기반으로 했을 때, 처리량이 가장 높은 것은 카프카이며, 응답 속도가 가장 빠른 것은 RabbitMQ
- 하지만, 처리량과 응답 속도를 같이 비교했을 때 가장 우수한 성능은 카프카

> 높은 확장성 

- 엔터프라이즈 환경의 기업들은 높은 처리량 요구 
- 처리량이 높은 시스템일지라도 한계가 존재 
  - 카프카는 손쉬운 확장 가능

> 고가용성

- 2013년, 클러스터 내 레플리케이션 기능 추가

> 내구성

- 프로듀서 -> 카프카로 메시지 전송 시, 프로듀서의 Acks 옵션을 조정하여 메시지의 내구성 강화 가능
- 카프카의 경우, 컨슈머가 메시지를 처리하더라도 설정된 시간 또는 Log의 크기만큼 로컬 디스크에 보관
  - 장애 발생 시에도 과거 메시지들을 통해 재처리 가능
- 여러 대의 브로커를 사용할 수 있으므로 브로커 1대가 중지되더라도 다른 브로커의 로컬 디스크에 저장된 메시지를 통해 복구 가능

> 개발 편의성

- 카프카는 메시지를 전송하는 역할인 프로듀서와 메시지를 가져오는 역할인 컨슈머가 완벽히 분리되어 동작하며 서로 영향 받지 않음
- 카프카 커넥트, 스키마 레지스트리 제공
  - 스키마 레지스트리 
    - 스키마를 정의하여 사용할 수 있도록 개발된 Application
    - https://always-kimkim.tistory.com/entry/kafka101-schema-registry
    - https://velog.io/@fj2008/%EC%B9%B4%ED%94%84%EC%B9%B4-%EC%8A%A4%ED%82%A4%EB%A7%88-%EB%A0%88%EC%A7%80%EC%8A%A4%ED%8A%B8%EB%A6%AC
  - 카프카 커넥트
    - 프로듀서와 컨슈머를 따로 개발하지 않고도 카프카와 연동해 손쉽게 소스와 싱크로 데이터를 보내고 받을 수 있는 별도의 Application
    - https://bagbokman.tistory.com/8
    
## 1.4. 카프카의 성장

> 리플리케이션 기능 추가 (v0.8)

- 0.8 버전부터 내부 카프카 클러스터에서 브로커의 장애가 발생해도 리플리케이션 기능으로 인하여 데이터 유실 없이 안정적으로 사용 가능

> 스키마 레지스트리 공개 (v.0.8.2)

- 카프카의 데이터 흐름은 대부분 브로드캐스트 방식이기 때문에 컨슈머 입장에서는 데이터를 전송하는 프로듀서를 일방적으로 신뢰할 수 밖에 없는 구조
- 프로듀서와 컨슈머 간에 서로 데이터 구조를 설명할 수 있는 스키마를 등록 지정해 사용
  - 스키마에 정의된 데이터만 송수신

> 카프카 커넥트 공개 (v.0.9)

- 카프카 커넥트를 이용해 별도의 코드 작성 없이도 다양한 프로토콜과 카프카 연동 가능

> 카프카 스트림즈 공개 (v.0.10)

- 실시간 처리에 대한 니즈를 충족하기 위해 2016년, 카프카 스트림즈 공개

> KSQL 공개

- 2017년, SQL 기반으로 실시간 처리 가능한 KSQL 공개
- 스트림 처리, 배치 처리 가능

> 주키퍼 의존성에서 해방 (v.3.0)

- 2021년 4월, 주키퍼 없이 동작 가능한 카프카 공개

## 1.5. 다양한 카프카의 사용 사례

![캡처](https://miro.medium.com/max/1400/0*twzMM1i4zLCilScl.)

> 데이터 파이프라인 : 넷플릭스 사례

- 사용자의 넷플릭스 비디오 시청 활동, 유저 인터페이스 사용 빈도, 에러 로그 등의 모든 이벤트를 데이터 파이프라인을 통하여 처리
  - 위와 같은 데이터를 수집하여 사용자의 경험을 예측하여 능동적으로 대응
  - 오류 발생 시 실시간 대응

> 데이터 통합 : 우버 사례

![캡처](http://eng.uber.com/wp-content/uploads/2016/08/image00.png)

- 운전자와 탑승자 앱으로부터 이벤트 데이터를 수집하고, 수집된 데이터는 카프카를 통해 다양한 다운스트림 컨슈머들에게 전달
  - 실시간 파이프라인
    - 디버깅
    - 알람
  - 배치 파이프 라인
    - Application 분석

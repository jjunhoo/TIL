## 카프카의 내부 동작 원리와 구현

### 4.1 카프카 리플리케이션 

- 고가용성 분산 스트리밍 플랫폼인 카프카는 데이터 파이프라인 정중앙에 위치하는 메인 허브 역할
- 카프카는 초기 설계 단계에서부터 일시적인 하드웨어 이슈 등으로 브로커 한두 대에서 장애가 발생하더라도 중앙 데이터 허브로서 안정적 서비스가 운영될 수 있도록 구상

#### 4.1.1 리플리케이션 동작 개요

- 카프카 리플리케이션 동작을 위해 토픽 생성 시 필수값으로 'replication factor' 옵션 설정 필요
- \* 예제 토픽 생성 - 토픽명 : peter-test01, 파티션 수 : 1, 리플리케이션 팩터 수 : 3 설정
> /kafka-topics.sh --bootstrap-server peter-kafka01.foo.bar:9092 --create --topic peter-test01 --partitions 1 --replication-factor 3


- \* 토픽 상세보기 
> /kafka-topics.sh --bootstrap-server peter-kafka01.foo.bar:9092 --topic peter-test01 --describe
````shell
// 출력 결과
Topic: peter-test01   PartitionCount: 1   ReplicationFactor: 3   Configs: segment.   bytes=1073741824
Topic: peter-test01   Partition: 0        Leader: 1      Replicas: 1,2,3      Isr: 1,2,3
````
   - Leader 
     - 파티션 0의 리더 : 1번 브로커 
   - Replicas
     - 리플리케이션은 브로커 1,2,3 에 저장
   - Isr
     - 동기화되고 있는 리플리케이션 : 브로커 1,2,3 
     

- \* 콘솔 프로듀서를 통한 토픽 전송 
> kafka-console.producer.sh --bootstrap-server peter-kafka01.foo.bar:9092 --topic peter-test01
> > test message1

- \* kafka-dump-log.sh 명령어를 통한 세그먼트 파일 내용 확인 

> kafka-dump-log.sh --print-data-log --files /data/kafka-logs/peter-test01-0/00000000000000000000.log
````shell
// 출력 결과
Dumping /data/kafka-logs/peter-test01-0/00000000000000000000.log
Starting offset: 0
baseOffset: 0 lastOffset: 0 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1
producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false
position: 0 CreateTime: 1601008070323 size: 81 magic: 2 compresscodec: NONE crc: 3417270022 isvalid: true
| offset: 0 CreateTime: 1601008070323 keysize: -1 valuesize: 13 sequence: -1 headerKeys:[]payload: test message1
````

- Starting offset: 0
  - 시작 오프셋 위치 : 0 
- count: 1
  - 메시지 카운트 : 1 
- payload: test message1
  - 프로듀서를 통해 보낸 메시지 

> peter-kafka01 뿐만아니라 peter-kafka02, peter-kafka03 에도 동일한 메시지가 저장되어 있음을 확인 가능 
> > 예제 기준으로 replication-factor 옵션을 3으로 설정하였으므로 2대의 브로커에 장애가 발생하더라도 나머지 1대를 통하여 운영 가능

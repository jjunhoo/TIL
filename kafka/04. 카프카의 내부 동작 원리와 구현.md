## 카프카의 내부 동작 원리와 구현

### 4.1 카프카 리플리케이션 

- 고가용성 분산 스트리밍 플랫폼인 카프카는 데이터 파이프라인 정중앙에 위치하는 메인 허브 역할
- 카프카는 초기 설계 단계에서부터 일시적인 하드웨어 이슈 등으로 브로커 한두 대에서 장애가 발생하더라도 중앙 데이터 허브로서 안정적 서비스가 운영될 수 있도록 구상

#### 4.1.1 리플리케이션 동작 개요

- 카프카 리플리케이션 동작을 위해 토픽 생성 시 필수값으로 'replication factor' 옵션 설정 필요
- \* 예제 토픽 생성 - 토픽명 : peter-test01, 파티션 수 : 1, 리플리케이션 팩터 수 : 3 설정
> /kafka-topics.sh --bootstrap-server peter-kafka01.foo.bar:9092 --create --topic peter-test01 --partitions 1 --replication-factor 3


- \* 토픽 상세보기 
> /kafka-topics.sh --bootstrap-server peter-kafka01.foo.bar:9092 --topic peter-test01 --describe
````shell
// 출력 결과
Topic: peter-test01   PartitionCount: 1   ReplicationFactor: 3   Configs: segment.   bytes=1073741824
Topic: peter-test01   Partition: 0        Leader: 1      Replicas: 1,2,3      Isr: 1,2,3
````
   - Leader 
     - 파티션 0의 리더 : 1번 브로커 
   - Replicas
     - 리플리케이션은 브로커 1,2,3 에 저장
   - Isr
     - 동기화되고 있는 리플리케이션 : 브로커 1,2,3 
     

- \* 콘솔 프로듀서를 통한 토픽 전송 
> kafka-console.producer.sh --bootstrap-server peter-kafka01.foo.bar:9092 --topic peter-test01
> > test message1

- \* kafka-dump-log.sh 명령어를 통한 세그먼트 파일 내용 확인 

> kafka-dump-log.sh --print-data-log --files /data/kafka-logs/peter-test01-0/00000000000000000000.log
````shell
// 출력 결과
Dumping /data/kafka-logs/peter-test01-0/00000000000000000000.log
Starting offset: 0
baseOffset: 0 lastOffset: 0 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1
producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false
position: 0 CreateTime: 1601008070323 size: 81 magic: 2 compresscodec: NONE crc: 3417270022 isvalid: true
| offset: 0 CreateTime: 1601008070323 keysize: -1 valuesize: 13 sequence: -1 headerKeys:[]payload: test message1
````

- Starting offset: 0
  - 시작 오프셋 위치 : 0 
- count: 1
  - 메시지 카운트 : 1 
- payload: test message1
  - 프로듀서를 통해 보낸 메시지 

> peter-kafka01 뿐만아니라 peter-kafka02, peter-kafka03 에도 동일한 메시지가 저장되어 있음을 확인 가능 
> > 예제 기준으로 replication-factor 옵션을 3으로 설정하였으므로 2대의 브로커에 장애가 발생하더라도 나머지 1대를 통하여 운영 가능


#### 4.1.2 리더와 팔로워

- 카프카는 내부적으로 모두 동일한 리플리케이션들을 리더와 팔로워로 구분
- 리더 : 리플리케이션 중 하나가 선정되며, 모든 읽기와 쓰기는 해당 리더를 통해서만 가능
  - 프로듀서 : 리더로 메시지 전송
  - 컨슈머 : 리더로부터 메시지 소비
- 팔로워 : 지속적으로 파티션의 리더가 새로운 메시지를 받았는지 확인하고, 새로운 메시지가 있다면 해당 메시지를 리더로부터 복제
  - 리더 이슈 발생 시 리더로 승격 

#### 4.1.3 복제 유지와 커밋

- 리더와 팔로워는 ISR (InSyncReplica) 이라는 논리적 그룹으로 묶여있음 
  1. ISR 그룹 안에 속한 팔로워들만이 새로운 리더 자격 조건 충족
  2. ISR 그룹 내 팔로워 중에서도 네트워크 오류, 브로커 장애 등 이슈가 없고, 리더와의 데이터 일치를 유지하는 팔로워들만이 새로운 리더 자격 조건 충족

> 리플리케이션 정상 동작 확인 주체
  - 리더 
> 리플리케이션 정상 동작 확인 기준 
  - ISR 내 모든 팔로워가 토픽 복제 완료 시 리더는 내부적으로 커밋 표시 
    - 마지막 커밋 오프셋 위치 : \* 하이워터마크 (high water mark)
    - 즉, 커밋이 되었다는 것은 리플리케이션 팩터 수의 모든 리플리케이션이 전부 메시지를 복제했음을 의미
    - 또한, 이렇게 커밋이 완료된 메시지만 컨슈머가 읽을 수 있음 (메시지 일관성 유지)
    - ![캡처](https://velog.velcdn.com/images/woorung/post/cc34aa1f-d547-469d-9ac1-636f78f15932/image.png)
      - 'test message2' 의 경우, 커밋되지 않은 상태이기 때문에 컨슈머에서 읽기 불

> 커밋되기 전 메시지를 컨슈머가 읽을 수 있는 경우 흐름

- ![캡처](https://velog.velcdn.com/images/woorung/post/6c9d6532-13a5-4c81-bd76-327bb6461b1d/image.png)

1. 컨슈머 A 가 peter-test01 토픽 컨슘
2. 컨슈머 A 는 perter-test01 토픽의 파티션 리더로부터 메시지를 읽어감 (읽어간 메시지는 test message1, 2 - 리플리케이션 동작 전)
3. peter-test01 토픽의 파티션 리더가 있는 브로커에 문제가 발생해 팔로워 중 하나가 새로운 리더로 선출
4. 프로듀서가 보낸 'test message2' 메시지가 팔로워들에게 리플리케이션 되지 않은 상태에서 새로운 리더가 선출됐으므로 새로운 리더는 'test message1' 메시지만 보유
5. 새로운 컨슈머 B 가 peter-test01 토픽 컨슘
6. 새로운 리더로부터 메시지를 읽어가지만, 'test message2' 는 유실되고, 'test message1' 만 컨슘

- 결과 
  - 컨슈머 A : 'test message1', 'test message2'
  - 컨슈머 B : 'test message1' 

- 커밋되지 않은 메시지를 컨슈머가 읽게된다면, 동일한 토픽의 파티션에서 컨슘했음에도 메시지 불일치 현상 발생

> 커밋 위치 확인 방법

- 모든 브로커는 재시작될 때, 커밋된 메시지를 유지하기 위해 로컬 디스크의 'replication-offset-checkpoint' 파일에 마지막 커밋 오프셋 위치 저장
````shell
cat /data/kafka-logs/replication-offset-checkpoint

> peter-test01 0 1 
````
- peter-test01 : 토픽명
- 0 : 파티션 번호
- 1 : 커밋된 오프셋 번호 (순차 증가)

4.1.4 리더와 팔로워의 단계별 리플리케이션 동작

## 카프카의 내부 동작 원리와 구현

### 4.1 카프카 리플리케이션 

- 고가용성 분산 스트리밍 플랫폼인 카프카는 데이터 파이프라인 정중앙에 위치하는 메인 허브 역할
- 카프카는 초기 설계 단계에서부터 일시적인 하드웨어 이슈 등으로 브로커 한두 대에서 장애가 발생하더라도 중앙 데이터 허브로서 안정적 서비스가 운영될 수 있도록 구상

#### 4.1.1 리플리케이션 동작 개요

- 카프카 리플리케이션 동작을 위해 토픽 생성 시 필수값으로 'replication factor' 옵션 설정 필요
- \* 예제 토픽 생성 - 토픽명 : peter-test01, 파티션 수 : 1, 리플리케이션 팩터 수 : 3 설정
> /kafka-topics.sh --bootstrap-server peter-kafka01.foo.bar:9092 --create --topic peter-test01 --partitions 1 --replication-factor 3


- \* 토픽 상세보기 
> /kafka-topics.sh --bootstrap-server peter-kafka01.foo.bar:9092 --topic peter-test01 --describe
````shell
// 출력 결과
Topic: peter-test01   PartitionCount: 1   ReplicationFactor: 3   Configs: segment.   bytes=1073741824
Topic: peter-test01   Partition: 0        Leader: 1      Replicas: 1,2,3      Isr: 1,2,3
````
   - Leader 
     - 파티션 0의 리더 : 1번 브로커 
   - Replicas
     - 리플리케이션은 브로커 1,2,3 에 저장
   - Isr
     - 동기화되고 있는 리플리케이션 : 브로커 1,2,3 
     

- \* 콘솔 프로듀서를 통한 토픽 전송 
> kafka-console.producer.sh --bootstrap-server peter-kafka01.foo.bar:9092 --topic peter-test01
> > test message1

- \* kafka-dump-log.sh 명령어를 통한 세그먼트 파일 내용 확인 

> kafka-dump-log.sh --print-data-log --files /data/kafka-logs/peter-test01-0/00000000000000000000.log
````shell
// 출력 결과
Dumping /data/kafka-logs/peter-test01-0/00000000000000000000.log
Starting offset: 0
baseOffset: 0 lastOffset: 0 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1
producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false
position: 0 CreateTime: 1601008070323 size: 81 magic: 2 compresscodec: NONE crc: 3417270022 isvalid: true
| offset: 0 CreateTime: 1601008070323 keysize: -1 valuesize: 13 sequence: -1 headerKeys:[]payload: test message1
````

- Starting offset: 0
  - 시작 오프셋 위치 : 0 
- count: 1
  - 메시지 카운트 : 1 
- payload: test message1
  - 프로듀서를 통해 보낸 메시지 

> peter-kafka01 뿐만아니라 peter-kafka02, peter-kafka03 에도 동일한 메시지가 저장되어 있음을 확인 가능 
> > 예제 기준으로 replication-factor 옵션을 3으로 설정하였으므로 2대의 브로커에 장애가 발생하더라도 나머지 1대를 통하여 운영 가능


#### 4.1.2 리더와 팔로워

- 카프카는 내부적으로 모두 동일한 리플리케이션들을 리더와 팔로워로 구분
- 리더 : 리플리케이션 중 하나가 선정되며, 모든 읽기와 쓰기는 해당 리더를 통해서만 가능
  - 프로듀서 : 리더로 메시지 전송
  - 컨슈머 : 리더로부터 메시지 소비
- 팔로워 : 지속적으로 파티션의 리더가 새로운 메시지를 받았는지 확인하고, 새로운 메시지가 있다면 해당 메시지를 리더로부터 복제
  - 리더 이슈 발생 시 리더로 승격 

#### 4.1.3 복제 유지와 커밋

- 리더와 팔로워는 ISR (InSyncReplica) 이라는 논리적 그룹으로 묶여있음 
  1. ISR 그룹 안에 속한 팔로워들만이 새로운 리더 자격 조건 충족
  2. ISR 그룹 내 팔로워 중에서도 네트워크 오류, 브로커 장애 등 이슈가 없고, 리더와의 데이터 일치를 유지하는 팔로워들만이 새로운 리더 자격 조건 충족

> 리플리케이션 정상 동작 확인 주체
  - 리더 
> 리플리케이션 정상 동작 확인 기준 
  - ISR 내 모든 팔로워가 토픽 복제 완료 시 리더는 내부적으로 커밋 표시 
    - 마지막 커밋 오프셋 위치 : \* 하이워터마크 (high water mark)
    - 즉, 커밋이 되었다는 것은 리플리케이션 팩터 수의 모든 리플리케이션이 전부 메시지를 복제했음을 의미
    - 또한, 이렇게 커밋이 완료된 메시지만 컨슈머가 읽을 수 있음 (메시지 일관성 유지)
    ![캡처](https://velog.velcdn.com/images/woorung/post/cc34aa1f-d547-469d-9ac1-636f78f15932/image.png)
      - 'test message2' 의 경우, 커밋되지 않은 상태이기 때문에 컨슈머에서 읽기 불

> 커밋되기 전 메시지를 컨슈머가 읽을 수 있는 경우 흐름

![캡처](https://velog.velcdn.com/images/woorung/post/6c9d6532-13a5-4c81-bd76-327bb6461b1d/image.png)

1. 컨슈머 A 가 peter-test01 토픽 컨슘
2. 컨슈머 A 는 perter-test01 토픽의 파티션 리더로부터 메시지를 읽어감 (읽어간 메시지는 test message1, 2 - 리플리케이션 동작 전)
3. peter-test01 토픽의 파티션 리더가 있는 브로커에 문제가 발생해 팔로워 중 하나가 새로운 리더로 선출
4. 프로듀서가 보낸 'test message2' 메시지가 팔로워들에게 리플리케이션 되지 않은 상태에서 새로운 리더가 선출됐으므로 새로운 리더는 'test message1' 메시지만 보유
5. 새로운 컨슈머 B 가 peter-test01 토픽 컨슘
6. 새로운 리더로부터 메시지를 읽어가지만, 'test message2' 는 유실되고, 'test message1' 만 컨슘

- 결과 
  - 컨슈머 A : 'test message1', 'test message2'
  - 컨슈머 B : 'test message1' 

- 커밋되지 않은 메시지를 컨슈머가 읽게된다면, 동일한 토픽의 파티션에서 컨슘했음에도 메시지 불일치 현상 발생

> 커밋 위치 확인 방법

- 모든 브로커는 재시작될 때, 커밋된 메시지를 유지하기 위해 로컬 디스크의 'replication-offset-checkpoint' 파일에 마지막 커밋 오프셋 위치 저장
````shell
cat /data/kafka-logs/replication-offset-checkpoint

> peter-test01 0 1 
````
- peter-test01 : 토픽명
- 0 : 파티션 번호
- 1 : 커밋된 오프셋 번호 (순차 증가)

#### 4.1.4 리더와 팔로워의 단계별 리플리케이션 동작

> 리더와 팔로워 간의 리플리케이션 동작 과정 1. (리더만 메시지를 저장하고, 나머지 팔로워들은 아직 리더에게 저장된 메시지를 리플리케이션 하기 전) 

![캡처](https://velog.velcdn.com/images/woorung/post/1a18b3f1-73fc-4f9b-b96d-188584a7a8ee/image.png)
- 1개의 파티션, 3개의 리플리케이션 팩터 
- 리더만 0번 오프셋에 'message1' 메시지를 갖고 있는 상태

> 리더와 팔로워 간의 리플리케이션 동작 과정 2. (팔로워들이 리더에게 0번 오프셋 메시지 가져오기(fetch) 요청을 보낸 후 새로운 메시지 'message1'을 리플리케이션 하는 과정)

![캡처](https://velog.velcdn.com/images/woorung/post/3d4bb80b-1044-4888-8180-5163308c693a/image.png)
- 리더는 모든 팔로워가 0번 오프셋 메시지를 리플리케이션 하기 위한 요청을 보낸 것을 알 수 있음
- 하지만, 리더는 팔로워들이 0번 오프셋에 대한 리플리케이션 동작을 성공했는지 실패했는지에 대한 여부는 알 수 없음
  - 카프카는 리더와 팔로워 사이에 ACK 를 주고 받는 통신이 없음
  - 리더와 팔로워 사이의 ACK 통신을 제거함으로서 동작 성능을 높임

> 리더와 팔로워 간의 리플리케이션 동작 과정 3. (ACK 통신 없이 안정적 리플리케이션 동작 과정 - 팔로워들의 리플리케이션 요청 오프셋을 통해 판단)

![캡처](https://velog.velcdn.com/images/woorung/post/75754f05-c378-4dc1-8220-ac95fda17954/image.png)
- 리더는 1번 오프셋 위치에 두 번째 새로운 메시지인 'message2'를 프로듀서로부터 수신 받은 뒤 저장
- 0번 오프셋에 대한 리플리케이션 동작을 마친 팔로워들은 리더에게 1번 오프셋에 대한 리플리케이션 요청
- 1번 오프셋에 대한 리플리케이션 요청을 받은 리더는 팔로워들의 0번 오프셋에 대한 리플리케이션 동작 성공 인지
- 오프셋 0에 대하여 커밋 표시 후 하이워터마크 증가
- 팔로워들로부터 1번 오프셋 메시지에 대한 리플리케이션 요청을 받은 리더는 응답에 0번 오프셋 'message1' 메시지가 커밋되었다는 내용을 포함하여 전달

- 팔로워가 0번 오프셋에 대해 리플리케이션을 성공하지 못한 경우, 팔로워는 1번 오프셋 리플리케이션 요청이 아닌 0번 오프셋 리플리케이션 재요청 
    - 리더는 팔로워들의 리플리케이션 요청 오프셋을 통해 팔로워들이 어느 위치까지 리플리케이션을 성공했는지 파악

> 리더와 팔로워 간의 리플리케이션 동작 과정 4. (리더가 보낸 커밋 메시지와 함께 리플리케이션 - 팔로워)

- 리더의 응답을 받은 모든 팔로워는 0번 오프셋 메시지 커밋 메시지를 확인 후 리더와 동일하게 커밋 표시
- 위 과정을 반복하며 리더와 팔로워 간의 메시지 최신 상태 유지

> 카프카 장점 1. ACK 통신 단계 제거를 통한 리더의 메시지 송수신 기능 집중 및 부하 최소화
> 카프카 장점 2. 리더가 Push 하는 방식이 아닌 팔로워가 Pull 하는 방식을 통한 리더 부하 최소화

#### 4.1.5 리더에포크와 복구

- 리더에포크 (LeaderEpoch)는 카프카의 파티션들이 복구 동작을 할 때 메시지의 일관성을 유지하기 위한 용도로 이용
  - 리더에포크는 복구 동작 시 하이워터마크를 대체하는 수단으로도 활용
- 리더에포크는 컨트롤러에 의해 관리되는 32비트 숫자로 표현
- 새로운 리더로 변경된 후 변경된 리더에 대한 정보가 팔로워들에게 전달

> 브로커가 복구 동작을 하는 데 리더에포크가 필요한 이유 

##### 4.1.5.1 리더에포크를 사용하지 않은 장애 복구 과정 

![캡처](https://velog.velcdn.com/images/woorung/post/d0971cca-47d0-4c53-a7aa-568f9c1eb497/image.png)
- 파티션 : 1 
- 리플리케이션 팩터 : 2
- min.insync.replicas : 1 

1. 리더는 프로듀서로부터 'message1' 메시지 수신 및 0번 오프셋에 메시지 저장, 팔로워는 리더에게 0번 오프셋에 대한 가져오기 요청 
2. 가져오기 요청을 통해 팔로워는 'message1' 메시지를 리더로부터 리플리케이션
3. 리더는 하이워터마크를 1로 올림
4. 리더는 프로듀서로부터 다음 메시지인 'message2' 메시지 수신 및 1번 오프셋에 메시지 저장
5. 팔로워는 다음 메시지인 'message2' 메시지를 리더에게 가져오기 요청, 리더의 응답을 통해 리더의 하이워터마크 변화 감지 및 팔로워 자신의 하이워터마크도 1로 올림
6. 팔로워는 1번 오프셋의 'message2' 메시지를 리더로부터 리플리케이션 
7. 팔로워는 2번 오프셋에 대한 요청을 리더에게 보내고, 요청을 받은 리더는 하이워터마크를 2로 올림
8. \* 팔로워는 2번 오프셋인 'message2' 메시지까지 리플리케이션을 완료했지만, 아직 리더로부터 하이워터마크를 2로 올리는 내용은 전달 받지 못함
9. 예상치 못한 장애로 팔로워 다운

![캡처](https://velog.velcdn.com/images/woorung/post/36824c4c-9b83-4574-bb1f-ff68bee93f32/image.png)

> 장애가 발생한 팔로워가 종료된 후 장애 처리를 완료한 상태 (장애에서 복구된 팔로워는 카프카 프로세스가 시작되면서 내부적으로 '메시지 복구 동작' 수행)

1. 팔로워는 자신이 갖고 있는 메시지들 중에서 자신의 워터마크보다 높은 메시지들은 신뢰할 수 없는 메시지로 판단하고 삭제 
   - 이에따라 해당 팔로워의 1번 오프셋 'message2' 메시지 삭제
2. 팔로워는 리더에게 1번 오프셋의 새로운 메시지에 대하여 가져오기 요청
3. 이 순간 리더였던 브로커가 예상치 못한 장애로 다운되며, 해당 파티션에 유일하게 남아 있던 팔로워가 새로운 리더로 승격
   - \* 리더인 브로커가 장애로 다운되며 팔로워가 'message2' 를 리플리케이션 하기 전 다운 되며 유일한 메시지 유실

![캡처](https://velog.velcdn.com/images/woorung/post/91d5c80c-04f0-4fab-bcfc-b43c7ae98ee5/image.png)

##### 4.1.5.2 리더에포크를 사용한 장애 복구 과정

> 리더에포크를 사용하지 않는 경우, 팔로워 장애 복구 후 카프카 프로세스가 시작되면서 복구 동작을 통해 자신의 하이워터마크보다 높은 메시지 즉시 삭제

> 리더에포크를 사용하는 경우, 팔로워 장애 복구 후 카프카 프로세스가 시작되면서 복구 동작을 통해 자신의 하이워터마크보다 높은 메시지에 대하여 리더에게 리더에포크 요청

![캡처](https://velog.velcdn.com/images/woorung/post/6890fc44-1300-4f5d-be01-e631d0646e6a/image.png)

1. 팔로워는 복구 동작을 하면서 리더에게 리더에포크 요청 
2. 요청을 받은 리더는 '1번 오프셋의 message2까지' 라는 정보를 팔로워에게 리더에포크 응답
3. \* 팔로워는 자신의 하이워터마크보다 높은 1번 오프셋의 'message2'를 삭제하지 않고, 리더의 응답을 확인한 후 'message2' 까지 자신의 하이워터마크를 상향 조정

![캡처](https://velog.velcdn.com/images/woorung/post/85b9cfd7-bbdb-4b6a-963c-f512dad2b0ac/image.png)

> 리더에포크를 사용하지 않는 경우, 그림 4-12 상황에서 팔로워가 'message2' 메시지를 갖고 있음에도 복구 과정에서 자신의 하이워터마크보다 높은 메시지를 삭제했기 때문에 'message2' 메시지 유실 발생

> 리더에포크를 사용하는 경우, 그림 4-12 과 같이 기존의 리더 브로커가 장애로 다운되기 전 팔로워가 리더에포크를 통해 팔로워의 하이워터마크를 올릴 수 있었기 때문에 메시지 유실 미발생  

### 4.2 컨트롤러

> 카프카 리더 선출 역할 담당 (브로커 실패 여부 모니터링 및 브로커 실패 감지 시 ISR 리스트 중 하나를 새로운 파티션 리더로 선출)

- 카프카 클러스터 중 하나의 브로커가 컨트롤러 역할 수행 
- 파티션의 ISR 리스트 중에서 리더 선출
- 리더 선출을 위한 ISR 리스트 정보는 가용성 보장을 위해 주키퍼에 저장
- 새로운 리더 선출 시 새로운 리더 정보를 주키퍼에 저장 
  - 변경 정보에 대해 모든 브로커에 전파
  - 클라이언트들이 재시도하는 시간 내에 리더 선출 작업 필요

> 예기치 않은 장애로 인한 리더 선출 과정 

| 토픽 이름          | peter-test02 | 
|------------------|--------------| 
| 파티션 수          | 1            | 
| 리플리케이션 팩터 수  | 2            | 
| 브로커 배치         | 1, 3번 브로커  |
| 현재 리더 위치       | 1번 브로커     |

- 0번 파티션의 리더가 있는 1번 브로커 강제 종료 후 새로운 파티션 리더가 선출되는 과정
![캡처](https://velog.velcdn.com/images/woorung/post/8920a8ac-f7eb-43b7-8332-74d17a1d7ab0/image.png)

1. 파티션 0번의 리더가 있는 브로커 1번 다운
2. 주키퍼는 1번 브로커와 연결이 끊어진 후, 0번 파티션의 ISR에서 변화 발생 감지
3. 컨트롤러는 주키퍼 워치를 통해 0번 파티션 변화 감지 및 해당 파티션 ISR 중 3번 브로커를 새로운 리더로 선출
4. 컨트롤러는 0번 파티션의 새로운 리더가 3번 브로커라는 정보를 주키퍼에 기록
5. 새로운 리더가 선출된 정보는 현재 활성화 상태인 모든 브로커에게 전파

> 카프카 버전 1.1.0 : 리더 선출 작업 속도 개선 

- 총 5개의 브로커, 각각 10,000개의 파티션이 있는 카프카 클러스터에서 리더 선출 성능 테스트 
  - 기존 6.5분 -> 30초로 단축 
  - 불필요 로깅 제거, 주키퍼 비동기 API 반영 

> 제어된 (관리자에 의한) 브로커 종료 과정에 의한 리더 선출 과정

![캡처](https://velog.velcdn.com/images/woorung/post/f83d53e1-b767-43f8-b156-bbecae74ac8e/image.png)

1. 관리자가 브로커 종료 명령어 실행 및 SIG_TERM 신호가 브로커에 전달
2. SIG_TERM 신호를 받은 브로커는 컨트롤러에게 리더 선출 필요 알림
3. 컨트롤러는 리더 선출 작업 진행 및 해당 정보를 주키퍼에 저장
4. 컨트롤러는 새로운 리더 정보를 다른 활성화된 브로커들에게 전파
5. 컨트롤러는 종료 요청을 보낸 브로커에게 정상 종료한다는 응답 
6. 컨트롤러로부터 정상 종료 응답을 받은 브로커는 캐시에 있는 내용을 디스크에 저장하고 종료

> 제어된 종료와 장애 발생에 의한 종료 차이 

- 다운타임
- 제어된 종료 시 카프카 내부적으로 파티션들의 다운타임 최소화 가능 
  - \* 브로커가 종료되기 전, 컨트롤러는 해당 브로커가 리더로 할당된 전체 파티션에 대해 리더 선출 작업을 진행
  - 제어된 종료의 경우, 브로커는 자신의 모든 로그를 디스크에 동기화한 뒤 종료되므로 재시작 시 로그 복구 시간 최소화 가능
    - server.properties 파일 내 controller.shutdown.enable = true 설정 필요 

### 4.3 로그 (로그 세그먼트)

- 카프카의 토픽으로 들어오는 메시지(레코드)는 세그먼트 (로그 세그먼트) 라는 파일에 저장
  - 메시지 내용, 메시지 Key, Value, Offset, 메시지 크기 등과 같은 정보 함께 저장 
  - 로그 세그먼트 파일들은 브로커의 로컬 디스크에 보관
  - 로그 세그먼트 파일 최대 크기 : 1GB
    - 1GB 초과 시 '롤링 전략' 적용
  - 로그 세그먼트 파일이 무한히 늘어날 경우를 대비한 관리 계획 필요
    - 로그 세그먼트 관리 방법 
      - 로그 세그먼트 삭제, 컴팩션 (Compaction)

#### 4.3.1 로그 세그먼트 삭제

- 로그 세그먼트 삭제 옵션 
  - server.properties 파일 내 log.cleanup.policy = delete 적용 필요 (Default)
  - 아래 2가지 방법으로 삭제 가능 
    - 보관 시간 : retention.ms 
    - 보관 크기 : retention.bytes

#### 4.3.2 로그 세그먼트 컴팩션

- 로그를 삭제하지 않고 컴팩션하여 보관 가능
  - \* 메시지의 키값을 기준으로 과거 정보는 중요하지 않고 가장 마지막 값이 필요한 경우 사용
- 현재 활성화된 세그먼트는 제외하고 나머지 세그먼트들을 대상으로 컴팩션 실행 
- 메시지 (레코드)의 키값을 기준으로 마지막 데이터만 보관
  - consumer offset 토픽 (컨슈머 그룹 정보) 이 대표적

> 로그 컴팩션 과정 

![캡처](https://velog.velcdn.com/images/woorung/post/8a74a6d4-96c1-458f-a765-b279eed51936/image.png)

> 로그 컴팩션 장점 

- 빠른 장애 복구 
  - 장애 복구 시 전체 로그를 복구하는 것이 아니라, 메시지의 키를 기준으로 최신의 상태만 복구하기 때문에 복구 시간 단축 가능
  - 하지만, 로그 컴팩션 과정에서 브로커의 과도한 입출력 부하 발생 가능 (브로커 리소스 모니터링 필요)

> 로그 컴팩션 관련 옵션

| 옵션    | 옵션 값 | 적용 범위 | 설명                                                                                           | 
|-------|------|------|----------------------------------------------------------------------------------------------| 
| cleanup.policy | compact | 토픽의 옵션으로 적용 | 토픽 레벨에서 로그 컴팩션을 설정할 때 적용하는 옵션                                                                |
| log.cleanup.policy | compact | 브로커의 설정 파일에 적용 | 브로커 레벨에서 로그 컴팩션을 설정할 때 적용하는 옵션                                                               | 
| log.cleaner.min.compaction.lag.ms | 0 | 브로커의 설정 파일에 적용 | 메세지가 기록된 후 컴팩션하기 전 경과되어야 할 최소 시간을 지정한다. 만약 이 옵션을 설정하지 않으면, 마지막 세그먼트를 제외하고 모든 세그먼트를 컴팩션할 수 있음 |
| log.cleaner.max.compaction.lag.ms | 9223372036854775807 | 브로커의 설정 파일에 적용 | 메세지가 기록된 후 컴팩션하기 전 경과되어야 할 최대 시간을 지정한다 |
| log.cleaner.min.cleanable.ratio | 0.5 | 브로커의 설정 파일에 적용 | 로그에서 압축이 되지 않은 부분을 더티라고 표현한다. 저체 로그 대비 더티의 비율이 50%가 넘으면 로그 컴팩션이 실행 |

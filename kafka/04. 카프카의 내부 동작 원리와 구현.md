## 카프카의 내부 동작 원리와 구현

### 4.1 카프카 리플리케이션 

- 고가용성 분산 스트리밍 플랫폼인 카프카는 데이터 파이프라인 정중앙에 위치하는 메인 허브 역할
- 카프카는 초기 설계 단계에서부터 일시적인 하드웨어 이슈 등으로 브로커 한두 대에서 장애가 발생하더라도 중앙 데이터 허브로서 안정적 서비스가 운영될 수 있도록 구상

#### 4.1.1 리플리케이션 동작 개요

- 카프카 리플리케이션 동작을 위해 토픽 생성 시 필수값으로 'replication factor' 옵션 설정 필요
- \* 예제 토픽 생성 - 토픽명 : peter-test01, 파티션 수 : 1, 리플리케이션 팩터 수 : 3 설정
> /kafka-topics.sh --bootstrap-server peter-kafka01.foo.bar:9092 --create --topic peter-test01 --partitions 1 --replication-factor 3


- \* 토픽 상세보기 
> /kafka-topics.sh --bootstrap-server peter-kafka01.foo.bar:9092 --topic peter-test01 --describe
````shell
// 출력 결과
Topic: peter-test01   PartitionCount: 1   ReplicationFactor: 3   Configs: segment.   bytes=1073741824
Topic: peter-test01   Partition: 0        Leader: 1      Replicas: 1,2,3      Isr: 1,2,3
````
   - Leader 
     - 파티션 0의 리더 : 1번 브로커 
   - Replicas
     - 리플리케이션은 브로커 1,2,3 에 저장
   - Isr
     - 동기화되고 있는 리플리케이션 : 브로커 1,2,3 
     

- \* 콘솔 프로듀서를 통한 토픽 전송 
> kafka-console.producer.sh --bootstrap-server peter-kafka01.foo.bar:9092 --topic peter-test01
> > test message1

- \* kafka-dump-log.sh 명령어를 통한 세그먼트 파일 내용 확인 

> kafka-dump-log.sh --print-data-log --files /data/kafka-logs/peter-test01-0/00000000000000000000.log
````shell
// 출력 결과
Dumping /data/kafka-logs/peter-test01-0/00000000000000000000.log
Starting offset: 0
baseOffset: 0 lastOffset: 0 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1
producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false
position: 0 CreateTime: 1601008070323 size: 81 magic: 2 compresscodec: NONE crc: 3417270022 isvalid: true
| offset: 0 CreateTime: 1601008070323 keysize: -1 valuesize: 13 sequence: -1 headerKeys:[]payload: test message1
````

- Starting offset: 0
  - 시작 오프셋 위치 : 0 
- count: 1
  - 메시지 카운트 : 1 
- payload: test message1
  - 프로듀서를 통해 보낸 메시지 

> peter-kafka01 뿐만아니라 peter-kafka02, peter-kafka03 에도 동일한 메시지가 저장되어 있음을 확인 가능 
> > 예제 기준으로 replication-factor 옵션을 3으로 설정하였으므로 2대의 브로커에 장애가 발생하더라도 나머지 1대를 통하여 운영 가능


#### 4.1.2 리더와 팔로워

- 카프카는 내부적으로 모두 동일한 리플리케이션들을 리더와 팔로워로 구분
- 리더 : 리플리케이션 중 하나가 선정되며, 모든 읽기와 쓰기는 해당 리더를 통해서만 가능
  - 프로듀서 : 리더로 메시지 전송
  - 컨슈머 : 리더로부터 메시지 소비
- 팔로워 : 지속적으로 파티션의 리더가 새로운 메시지를 받았는지 확인하고, 새로운 메시지가 있다면 해당 메시지를 리더로부터 복제
  - 리더 이슈 발생 시 리더로 승격 

#### 4.1.3 복제 유지와 커밋

- 리더와 팔로워는 ISR (InSyncReplica) 이라는 논리적 그룹으로 묶여있음 
  1. ISR 그룹 안에 속한 팔로워들만이 새로운 리더 자격 조건 충족
  2. ISR 그룹 내 팔로워 중에서도 네트워크 오류, 브로커 장애 등 이슈가 없고, 리더와의 데이터 일치를 유지하는 팔로워들만이 새로운 리더 자격 조건 충족

> 리플리케이션 정상 동작 확인 주체
  - 리더 
> 리플리케이션 정상 동작 확인 기준 
  - ISR 내 모든 팔로워가 토픽 복제 완료 시 리더는 내부적으로 커밋 표시 
    - 마지막 커밋 오프셋 위치 : \* 하이워터마크 (high water mark)
    - 즉, 커밋이 되었다는 것은 리플리케이션 팩터 수의 모든 리플리케이션이 전부 메시지를 복제했음을 의미
    - 또한, 이렇게 커밋이 완료된 메시지만 컨슈머가 읽을 수 있음 (메시지 일관성 유지)
    - ![캡처](https://velog.velcdn.com/images/woorung/post/cc34aa1f-d547-469d-9ac1-636f78f15932/image.png)
      - 'test message2' 의 경우, 커밋되지 않은 상태이기 때문에 컨슈머에서 읽기 불

> 커밋되기 전 메시지를 컨슈머가 읽을 수 있는 경우 흐름

- ![캡처](https://velog.velcdn.com/images/woorung/post/6c9d6532-13a5-4c81-bd76-327bb6461b1d/image.png)

1. 컨슈머 A 가 peter-test01 토픽 컨슘
2. 컨슈머 A 는 perter-test01 토픽의 파티션 리더로부터 메시지를 읽어감 (읽어간 메시지는 test message1, 2 - 리플리케이션 동작 전)
3. peter-test01 토픽의 파티션 리더가 있는 브로커에 문제가 발생해 팔로워 중 하나가 새로운 리더로 선출
4. 프로듀서가 보낸 'test message2' 메시지가 팔로워들에게 리플리케이션 되지 않은 상태에서 새로운 리더가 선출됐으므로 새로운 리더는 'test message1' 메시지만 보유
5. 새로운 컨슈머 B 가 peter-test01 토픽 컨슘
6. 새로운 리더로부터 메시지를 읽어가지만, 'test message2' 는 유실되고, 'test message1' 만 컨슘

- 결과 
  - 컨슈머 A : 'test message1', 'test message2'
  - 컨슈머 B : 'test message1' 

- 커밋되지 않은 메시지를 컨슈머가 읽게된다면, 동일한 토픽의 파티션에서 컨슘했음에도 메시지 불일치 현상 발생

> 커밋 위치 확인 방법

- 모든 브로커는 재시작될 때, 커밋된 메시지를 유지하기 위해 로컬 디스크의 'replication-offset-checkpoint' 파일에 마지막 커밋 오프셋 위치 저장
````shell
cat /data/kafka-logs/replication-offset-checkpoint

> peter-test01 0 1 
````
- peter-test01 : 토픽명
- 0 : 파티션 번호
- 1 : 커밋된 오프셋 번호 (순차 증가)

#### 4.1.4 리더와 팔로워의 단계별 리플리케이션 동작

> 리더와 팔로워 간의 리플리케이션 동작 과정 1. (리더만 메시지를 저장하고, 나머지 팔로워들은 아직 리더에게 저장된 메시지를 리플리케이션 하기 전) 

- ![캡처](https://velog.velcdn.com/images/woorung/post/1a18b3f1-73fc-4f9b-b96d-188584a7a8ee/image.png)
- 1개의 파티션, 3개의 리플리케이션 팩터 
- 리더만 0번 오프셋에 'message1' 메시지를 갖고 있는 상태

> 리더와 팔로워 간의 리플리케이션 동작 과정 2. (팔로워들이 리더에게 0번 오프셋 메시지 가져오기(fetch) 요청을 보낸 후 새로운 메시지 'message1'을 리플리케이션 하는 과정)

- ![캡처](https://velog.velcdn.com/images/woorung/post/3d4bb80b-1044-4888-8180-5163308c693a/image.png)
- 리더는 모든 팔로워가 0번 오프셋 메시지를 리플리케이션 하기 위한 요청을 보낸 것을 알 수 있음
- 하지만, 리더는 팔로워들이 0번 오프셋에 대한 리플리케이션 동작을 성공했는지 실패했는지에 대한 여부는 알 수 없음
  - 카프카는 리더와 팔로워 사이에 ACK 를 주고 받는 통신이 없음
  - 리더와 팔로워 사이의 ACK 통신을 제거함으로서 동작 성능을 높임

> 리더와 팔로워 간의 리플리케이션 동작 과정 3. (ACK 통신 없이 안정적 리플리케이션 동작 과정 - 팔로워들의 리플리케이션 요청 오프셋을 통해 판단)

- ![캡처](https://velog.velcdn.com/images/woorung/post/75754f05-c378-4dc1-8220-ac95fda17954/image.png)
- 리더는 1번 오프셋 위치에 두 번째 새로운 메시지인 'message2'를 프로듀서로부터 수신 받은 뒤 저장
- 0번 오프셋에 대한 리플리케이션 동작을 마친 팔로워들은 리더에게 1번 오프셋에 대한 리플리케이션 요청
- 1번 오프셋에 대한 리플리케이션 요청을 받은 리더는 팔로워들의 0번 오프셋에 대한 리플리케이션 동작 성공 인지
- 오프셋 0에 대하여 커밋 표시 후 하이워터마크 증가
- 팔로워들로부터 1번 오프셋 메시지에 대한 리플리케이션 요청을 받은 리더는 응답에 0번 오프셋 'message1' 메시지가 커밋되었다는 내용을 포함하여 전달

- 팔로워가 0번 오프셋에 대해 리플리케이션을 성공하지 못한 경우, 팔로워는 1번 오프셋 리플리케이션 요청이 아닌 0번 오프셋 리플리케이션 재요청 
    - 리더는 팔로워들의 리플리케이션 요청 오프셋을 통해 팔로워들이 어느 위치까지 리플리케이션을 성공했는지 파악

> 리더와 팔로워 간의 리플리케이션 동작 과정 4. (리더가 보낸 커밋 메시지와 함께 리플리케이션 - 팔로워)

- 리더의 응답을 받은 모든 팔로워는 0번 오프셋 메시지 커밋 메시지를 확인 후 리더와 동일하게 커밋 표시
- 위 과정을 반복하며 리더와 팔로워 간의 메시지 최신 상태 유지

> 카프카 장점 1. ACK 통신 단계 제거를 통한 리더의 메시지 송수신 기능 집중 및 부하 최소화
> 카프카 장점 2. 리더가 Push 하는 방식이 아닌 팔로워가 Pull 하는 방식을 통한 리더 부하 최소화

#### 4.1.5 리더에포크와 복구

- 리더에포크 (LeaderEpoch)는 카프카의 파티션들이 복구 동작을 할 때 메시지의 일관성을 유지하기 위한 용도로 이용
  - 리더에포크는 복구 동작 시 하이워터마크를 대체하는 수단으로도 활용
- 리더에포크는 컨트롤러에 의해 관리되는 32비트 숫자로 표현
- 새로운 리더로 변경된 후 변경된 리더에 대한 정보가 팔로워들에게 전달

> 브로커가 복구 동작을 하는 데 리더에포크가 필요한 이유 

##### 4.1.5.1 리더에포크를 사용하지 않은 장애 복구 과정 

![캡처](https://velog.velcdn.com/images/woorung/post/d0971cca-47d0-4c53-a7aa-568f9c1eb497/image.png)
- 파티션 : 1 
- 리플리케이션 팩터 : 2
- min.insync.replicas : 1 

1. 리더는 프로듀서로부터 'message1' 메시지 수신 및 0번 오프셋에 메시지 저장, 팔로워는 리더에게 0번 오프셋에 대한 가져오기 요청 
2. 가져오기 요청을 통해 팔로워는 'message1' 메시지를 리더로부터 리플리케이션
3. 리더는 하이워터마크를 1로 올림
4. 리더는 프로듀서로부터 다음 메시지인 'message2' 메시지 수신 및 1번 오프셋에 메시지 저장
5. 팔로워는 다음 메시지인 'message2' 메시지를 리더에게 가져오기 요청, 리더의 응답을 통해 리더의 하이워터마크 변화 감지 및 팔로워 자신의 하이워터마크도 1로 올림
6. 팔로워는 1번 오프셋의 'message2' 메시지를 리더로부터 리플리케이션 
7. 팔로워는 2번 오프셋에 대한 요청을 리더에게 보내고, 요청을 받은 리더는 하이워터마크를 2로 올림
8. \* 팔로워는 2번 오프셋인 'message2' 메시지까지 리플리케이션을 완료했지만, 아직 리더로부터 하이워터마크를 2로 올리는 내용은 전달 받지 못함
9. 예상치 못한 장애로 팔로워 다운

![캡처](https://velog.velcdn.com/images/woorung/post/36824c4c-9b83-4574-bb1f-ff68bee93f32/image.png)

> 장애가 발생한 팔로워가 종료된 후 장애 처리를 완료한 상태 (장애에서 복구된 팔로워는 카프카 프로세스가 시작되면서 내부적으로 '메시지 복구 동작' 수행)

1. 팔로워는 자신이 갖고 있는 메시지들 중에서 자신의 워터마크보다 높은 메시지들은 신뢰할 수 없는 메시지로 판단하고 삭제 
   - 이에따라 해당 팔로워의 1번 오프셋 'message2' 메시지 삭제
2. 팔로워는 리더에게 1번 오프셋의 새로운 메시지에 대하여 가져오기 요청
3. 이 순간 리더였던 브로커가 예상치 못한 장애로 다운되며, 해당 파티션에 유일하게 남아 있던 팔로워가 새로운 리더로 승격
   - \* 리더인 브로커가 장애로 다운되며 팔로워가 'message2' 를 리플리케이션 하기 전 다운 되며 유일한 메시지 유실

![캡처](https://velog.velcdn.com/images/woorung/post/91d5c80c-04f0-4fab-bcfc-b43c7ae98ee5/image.png)

##### 4.1.5.2 리더에포크를 사용한 장애 복구 과정

> 리더에포크를 사용하지 않는 경우, 팔로워 장애 복구 후 카프카 프로세스가 시작되면서 복구 동작을 통해 자신의 하이워터마크보다 높은 메시지 즉시 삭제

> 리더에포크를 사용하는 경우, 팔로워 장애 복구 후 카프카 프로세스가 시작되면서 복구 동작을 통해 자신의 하이워터마크보다 높은 메시지에 대하여 리더에게 리더에포크 요청

![캡처](https://velog.velcdn.com/images/woorung/post/6890fc44-1300-4f5d-be01-e631d0646e6a/image.png)

1. 팔로워는 복구 동작을 하면서 리더에게 리더에포크 요청 
2. 요청을 받은 리더는 '1번 오프셋의 message2까지' 라는 정보를 팔로워에게 리더에포크 응답
3. \* 팔로워는 자신의 하이워터마크보다 높은 1번 오프셋의 'message2'를 삭제하지 않고, 리더의 응답을 확인한 후 'message2' 까지 자신의 하이워터마크를 상향 조정

![캡처](https://velog.velcdn.com/images/woorung/post/85b9cfd7-bbdb-4b6a-963c-f512dad2b0ac/image.png)

> 리더에포크를 사용하지 않는 경우, 그림 4-12 상황에서 팔로워가 'message2' 메시지를 갖고 있음에도 복구 과정에서 자신의 하이워터마크보다 높은 메시지를 삭제했기 때문에 'message2' 메시지 유실 발생

> 리더에포크를 사용하는 경우, 그림 4-12 과 같이 기존의 리더 브로커가 장애로 다운되기 전 팔로워가 리더에포크를 통해 팔로워의 하이워터마크를 올릴 수 있었기 때문에 메시지 유실 미발생  

##### 4.1.5.3 리더에포크를 적용하지 않았을 때 발생 가능한 상황

![캡처](https://velog.velcdn.com/images/woorung/post/328971b4-1885-4a7d-a476-e34093309e6e/image.png)

> 상황 1. 리더만 오프셋 1까지 저장, 팔로워는 아직 1번 오프셋 메시지에 대해 리플리케이션을 완료하지 못한 상황 가정

> 상황 2. 브로커들의 장애로 인하여 리더와 팔로워 모두 다운됐다는 가정

![캡처](https://velog.velcdn.com/images/woorung/post/7b936ce0-1d21-42d3-aebc-65b7f01aa610/image.png)

> 상황 3. 브로커가 모두 장애로 종료된 후 팔로워 브로커만 장애에서 복구된 상태 

1. 팔로워였던 브로커가 장애에서 먼저 복구
2. 'peter-test01' 토픽의 0번 파티션에 리더가 없으므로 복구된 팔로워가 새로운 리더로 승격
3. 새로운 리더는 프로듀서로부터 다음 메시지 'message3' 을 전달받고 1번 오프셋에 저장 및 자신의 하이워터마크 상향 조정

![캡처](https://velog.velcdn.com/images/woorung/post/230f9a58-6e9d-4c18-a16f-cd6af19473fe/image.png)

> 상황 4. 구 리더였던 브로커 복구된 상태

1. 구 리더였던 브로커가 장애에서 복구
2. 'peter-test01' 토픽의 0번 파티션에 이미 리더가 있으므로, 복구된 브로커는 팔로워 롤 수행
3. 리더와 메시지 정합성 확인을 위해 자신의 하이워터마크를 비교해보니 리더의 하이워터마크와 일치하므로, 브로커는 자신이 갖고 있던 메시지를 삭제 미수행
   - **\* 리더와 팔로워 모두 오프셋이 1이지만, 리더의 오프셋1 메시지 (message2) 와 팔로워의 오프셋1 메시지 (message3) 내용은 다름**
4. 리더는 프로듀서로부터 'message4' 메시지를 수신하여 오프셋2의 위치에 저장
5. 팔로워는 오프셋2인 'message4' 를 리플리케이션하기 위해 준비

##### 4.1.5.4 리더에포크 사용 시 구 리더 장애 복구 과정

![캡처](https://velog.velcdn.com/images/woorung/post/46735bbf-2c0c-47d1-bd6e-142d4bd744be/image.png)

> 상황 : 그림 4-14 이후 팔로워가 먼저 복구되어 뉴리더로 선출, 구 리더였던 브로커가 장애에서 복구된 상태

1. 구 리더였던 브로커가 장애에서 복구
2. 'peter-test01' 토픽의 0번 파티션에 이미 리더가 있고, 자신은 팔로워 롤 수행
3. 팔로워는 뉴리더에게 리더에포크 요청
4. 뉴리더는 0번 오프셋까지 유효 응답 
5. 팔로워는 메시지 일관성을 위해 로컬 파일에서 1번 오프셋인 'message2' 삭제
    - **\* 팔로워는 쓰기 권한이 없으므로, 리더에게 message2 추가 불가**
6. 팔로워는 리더로부터 1번 오프셋인 'message3' 리플리케이션 준비

#### 4.1.5.5 리더에포크 실습

> 토픽은 chaptor4-topic03 이고, 파티션 수는 1, 리플리케이션 팩터 수는 2로 진행

1. 토픽 생성
![캡처](https://velog.velcdn.com/images/woorung/post/3e8e4446-c3ac-4316-a73c-740161436517/image.png)

2. 토픽 상세보기를 통해 리더 확인
![캡처](https://velog.velcdn.com/images/woorung/post/fe51b6f0-b66b-455a-8bb7-e5670650656f/image.png)

````shell
0
1    -> 1은 현재의 리더에포크 번호
0 0  -> 첫 번째 0은 리더에포크 번호, 두 번째 0은 최종 커밋 후 새로운 메세지를 전송받게 될 오프셋 번호
````

![캡처](https://velog.velcdn.com/images/woorung/post/bb5cea6a-9751-4f02-b8f2-cbe27d91cda5/image.png)

1. 파티션 변화를 주기 위한 메시지 전송 

![캡처](https://velog.velcdn.com/images/woorung/post/cdf59ca1-a677-4515-b6bb-705c0a2a1725/image.png)

2. 리더에포크 상태 확인
    - 내용 변경 없음

![캡처](https://velog.velcdn.com/images/woorung/post/c23cbf5d-ad67-4e06-94c3-62176c12ff64/image.png)

3. 메시지 전송 후 리더에포크 상태

![캡처](https://velog.velcdn.com/images/woorung/post/7aa4f1c0-3c2a-4f67-a98c-a50fb04c2da5/image.png)

4. 새로운 리더 선출을 위한 기존 리더 브로커 강제 종료 

- sudo systemctl stop kafka-server (중지)
- sudo systemctl status kafka-server (상태)

![캡처](https://velog.velcdn.com/images/woorung/post/4b26dd23-a364-4e3b-9b2f-6cafa1f3f73b/image.png)

````shell
0
2     -> 현재의 리더에포크 번호, 브로커3이 종료되면서 새로운 리더 선출이 있었고, 리더에포크는 1에서 2로 변경(리더가 변경될 때마다 숫자 증가)
0 0   -> 첫 번째 0은 리더에포크 번호, 두 번째 0은 최종 커밋 후 새로운 메세지를 전송하게 될 오프셋 번호 (리더에포크 1 시점)
1 1   -> 첫 번째 1은 리더에포크 번호, 두 번째 1은 최종 커밋 후 새로운 메세지를 전송받게 될 오프셋 번호 (리더에포크 2 시점)
````

5. 기존 리더 브로커 강제 종료 및 새로운 리더 선출 후 리더에포크 상태

![캡처](https://velog.velcdn.com/images/woorung/post/74869be1-78e3-419a-a644-73114c8b127e/image.png)

- 리더에포크는 새로운 리더가 선출되었으므로 1증가 (1 -> 2)
- 리더에포크 번호가 1이었을 때의 시점 기준으로 가장 마지막에 커밋된 후 새로 메시지를 받게 될 오프셋 번호 기록 (0 0)
- 가장 마지막에 커밋된 오프셋 번호가 0이므로 카프카는 새로 메시지를 전송 받도록 준비된 오프셋 번호 1을 'leader-epoch-checkpoint' 파일에 기록 (1 1)
- 다운되었던 브로커 3은 'leader-epoch-checkpoint' 파일에 기록된 정보를 이용해 복구 동작 수행
- 구 리더 (브로커 3)는 종료 직전 마지막 리더에포크 번호가 1이므로 뉴리더 (브로커 1) 에게 1번 리더에포크에 대한 요청 
  - 뉴리더 (브로커 1)은 1번 리더에포크의 최종 커밋 후 준비된 오프셋 위치가 1이라는 응답
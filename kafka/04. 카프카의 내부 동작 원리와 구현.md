## 카프카의 내부 동작 원리와 구현

### 4.1 카프카 리플리케이션 

- 고가용성 분산 스트리밍 플랫폼인 카프카는 데이터 파이프라인 정중앙에 위치하는 메인 허브 역할
- 카프카는 초기 설계 단계에서부터 일시적인 하드웨어 이슈 등으로 브로커 한두 대에서 장애가 발생하더라도 중앙 데이터 허브로서 안정적 서비스가 운영될 수 있도록 구상

#### 4.1.1 리플리케이션 동작 개요

- 카프카 리플리케이션 동작을 위해 토픽 생성 시 필수값으로 'replication factor' 옵션 설정 필요
- \* 예제 토픽 생성 - 토픽명 : peter-test01, 파티션 수 : 1, 리플리케이션 팩터 수 : 3 설정
> /kafka-topics.sh --bootstrap-server peter-kafka01.foo.bar:9092 --create --topic peter-test01 --partitions 1 --replication-factor 3


- \* 토픽 상세보기 
> /kafka-topics.sh --bootstrap-server peter-kafka01.foo.bar:9092 --topic peter-test01 --describe
````shell
// 출력 결과
Topic: peter-test01   PartitionCount: 1   ReplicationFactor: 3   Configs: segment.   bytes=1073741824
Topic: peter-test01   Partition: 0        Leader: 1      Replicas: 1,2,3      Isr: 1,2,3
````
   - Leader 
     - 파티션 0의 리더 : 1번 브로커 
   - Replicas
     - 리플리케이션은 브로커 1,2,3 에 저장
   - Isr
     - 동기화되고 있는 리플리케이션 : 브로커 1,2,3 
     

- \* 콘솔 프로듀서를 통한 토픽 전송 
> kafka-console.producer.sh --bootstrap-server peter-kafka01.foo.bar:9092 --topic peter-test01
> > test message1

- \* kafka-dump-log.sh 명령어를 통한 세그먼트 파일 내용 확인 

> kafka-dump-log.sh --print-data-log --files /data/kafka-logs/peter-test01-0/00000000000000000000.log
````shell
// 출력 결과
Dumping /data/kafka-logs/peter-test01-0/00000000000000000000.log
Starting offset: 0
baseOffset: 0 lastOffset: 0 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1
producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false
position: 0 CreateTime: 1601008070323 size: 81 magic: 2 compresscodec: NONE crc: 3417270022 isvalid: true
| offset: 0 CreateTime: 1601008070323 keysize: -1 valuesize: 13 sequence: -1 headerKeys:[]payload: test message1
````

- Starting offset: 0
  - 시작 오프셋 위치 : 0 
- count: 1
  - 메시지 카운트 : 1 
- payload: test message1
  - 프로듀서를 통해 보낸 메시지 

> peter-kafka01 뿐만아니라 peter-kafka02, peter-kafka03 에도 동일한 메시지가 저장되어 있음을 확인 가능 
> > 예제 기준으로 replication-factor 옵션을 3으로 설정하였으므로 2대의 브로커에 장애가 발생하더라도 나머지 1대를 통하여 운영 가능


#### 4.1.2 리더와 팔로워

- 카프카는 내부적으로 모두 동일한 리플리케이션들을 리더와 팔로워로 구분
- 리더 : 리플리케이션 중 하나가 선정되며, 모든 읽기와 쓰기는 해당 리더를 통해서만 가능
  - 프로듀서 : 리더로 메시지 전송
  - 컨슈머 : 리더로부터 메시지 소비
- 팔로워 : 지속적으로 파티션의 리더가 새로운 메시지를 받았는지 확인하고, 새로운 메시지가 있다면 해당 메시지를 리더로부터 복제
  - 리더 이슈 발생 시 리더로 승격 

#### 4.1.3 복제 유지와 커밋

- 리더와 팔로워는 ISR (InSyncReplica) 이라는 논리적 그룹으로 묶여있음 
  1. ISR 그룹 안에 속한 팔로워들만이 새로운 리더 자격 조건 충족
  2. ISR 그룹 내 팔로워 중에서도 네트워크 오류, 브로커 장애 등 이슈가 없고, 리더와의 데이터 일치를 유지하는 팔로워들만이 새로운 리더 자격 조건 충족

> 리플리케이션 정상 동작 확인 주체
  - 리더 
> 리플리케이션 정상 동작 확인 기준 
  - ISR 내 모든 팔로워가 토픽 복제 완료 시 리더는 내부적으로 커밋 표시 
    - 마지막 커밋 오프셋 위치 : \* 하이워터마크 (high water mark)
    - 즉, 커밋이 되었다는 것은 리플리케이션 팩터 수의 모든 리플리케이션이 전부 메시지를 복제했음을 의미
    - 또한, 이렇게 커밋이 완료된 메시지만 컨슈머가 읽을 수 있음 (메시지 일관성 유지)
    - ![캡처](https://velog.velcdn.com/images/woorung/post/cc34aa1f-d547-469d-9ac1-636f78f15932/image.png)
      - 'test message2' 의 경우, 커밋되지 않은 상태이기 때문에 컨슈머에서 읽기 불

> 커밋되기 전 메시지를 컨슈머가 읽을 수 있는 경우 흐름

- ![캡처](https://velog.velcdn.com/images/woorung/post/6c9d6532-13a5-4c81-bd76-327bb6461b1d/image.png)

1. 컨슈머 A 가 peter-test01 토픽 컨슘
2. 컨슈머 A 는 perter-test01 토픽의 파티션 리더로부터 메시지를 읽어감 (읽어간 메시지는 test message1, 2 - 리플리케이션 동작 전)
3. peter-test01 토픽의 파티션 리더가 있는 브로커에 문제가 발생해 팔로워 중 하나가 새로운 리더로 선출
4. 프로듀서가 보낸 'test message2' 메시지가 팔로워들에게 리플리케이션 되지 않은 상태에서 새로운 리더가 선출됐으므로 새로운 리더는 'test message1' 메시지만 보유
5. 새로운 컨슈머 B 가 peter-test01 토픽 컨슘
6. 새로운 리더로부터 메시지를 읽어가지만, 'test message2' 는 유실되고, 'test message1' 만 컨슘

- 결과 
  - 컨슈머 A : 'test message1', 'test message2'
  - 컨슈머 B : 'test message1' 

- 커밋되지 않은 메시지를 컨슈머가 읽게된다면, 동일한 토픽의 파티션에서 컨슘했음에도 메시지 불일치 현상 발생

> 커밋 위치 확인 방법

- 모든 브로커는 재시작될 때, 커밋된 메시지를 유지하기 위해 로컬 디스크의 'replication-offset-checkpoint' 파일에 마지막 커밋 오프셋 위치 저장
````shell
cat /data/kafka-logs/replication-offset-checkpoint

> peter-test01 0 1 
````
- peter-test01 : 토픽명
- 0 : 파티션 번호
- 1 : 커밋된 오프셋 번호 (순차 증가)

#### 4.1.4 리더와 팔로워의 단계별 리플리케이션 동작

> 리더와 팔로워 간의 리플리케이션 동작 과정 1. (리더만 메시지를 저장하고, 나머지 팔로워들은 아직 리더에게 저장된 메시지를 리플리케이션 하기 전) 

- ![캡처](https://velog.velcdn.com/images/woorung/post/1a18b3f1-73fc-4f9b-b96d-188584a7a8ee/image.png)
- 1개의 파티션, 3개의 리플리케이션 팩터 
- 리더만 0번 오프셋에 'message1' 메시지를 갖고 있는 상태

> 리더와 팔로워 간의 리플리케이션 동작 과정 2. (팔로워들이 리더에게 0번 오프셋 메시지 가져오기(fetch) 요청을 보낸 후 새로운 메시지 'message1'을 리플리케이션 하는 과정)

- ![캡처](https://velog.velcdn.com/images/woorung/post/3d4bb80b-1044-4888-8180-5163308c693a/image.png)
- 리더는 모든 팔로워가 0번 오프셋 메시지를 리플리케이션 하기 위한 요청을 보낸 것을 알 수 있음
- 하지만, 리더는 팔로워들이 0번 오프셋에 대한 리플리케이션 동작을 성공했는지 실패했는지에 대한 여부는 알 수 없음
  - 카프카는 리더와 팔로워 사이에 ACK 를 주고 받는 통신이 없음
  - 리더와 팔로워 사이의 ACK 통신을 제거함으로서 동작 성능을 높임

> 리더와 팔로워 간의 리플리케이션 동작 과정 3. (ACK 통신 없이 안정적 리플리케이션 동작 과정 - 팔로워들의 리플리케이션 요청 오프셋을 통해 판단)

- ![캡처](https://velog.velcdn.com/images/woorung/post/75754f05-c378-4dc1-8220-ac95fda17954/image.png)
- 리더는 1번 오프셋 위치에 두 번째 새로운 메시지인 'message2'를 프로듀서로부터 수신 받은 뒤 저장
- 0번 오프셋에 대한 리플리케이션 동작을 마친 팔로워들은 리더에게 1번 오프셋에 대한 리플리케이션 요청
- 1번 오프셋에 대한 리플리케이션 요청을 받은 리더는 팔로워들의 0번 오프셋에 대한 리플리케이션 동작 성공 인지
- 오프셋 0에 대하여 커밋 표시 후 하이워터마크 증가
- 팔로워들로부터 1번 오프셋 메시지에 대한 리플리케이션 요청을 받은 리더는 응답에 0번 오프셋 'message1' 메시지가 커밋되었다는 내용을 포함하여 전달

- 팔로워가 0번 오프셋에 대해 리플리케이션을 성공하지 못한 경우, 팔로워는 1번 오프셋 리플리케이션 요청이 아닌 0번 오프셋 리플리케이션 재요청 
    - 리더는 팔로워들의 리플리케이션 요청 오프셋을 통해 팔로워들이 어느 위치까지 리플리케이션을 성공했는지 파악

> 리더와 팔로워 간의 리플리케이션 동작 과정 4. (리더가 보낸 커밋 메시지와 함께 리플리케이션 - 팔로워)

- 리더의 응답을 받은 모든 팔로워는 0번 오프셋 메시지 커밋 메시지를 확인 후 리더와 동일하게 커밋 표시
- 위 과정을 반복하며 리더와 팔로워 간의 메시지 최신 상태 유지

> 카프카 장점 1. ACK 통신 단계 제거를 통한 리더의 메시지 송수신 기능 집중 및 부하 최소화
> 카프카 장점 2. 리더가 Push 하는 방식이 아닌 팔로워가 Pull 하는 방식을 통한 리더 부하 최소화

#### 4.1.5 리더에포크와 복구

- 리더에포크 (LeaderEpoch)는 카프카의 파티션들이 복구 동작을 할 때 메시지의 일관성을 유지하기 위한 용도로 이용
  - 리더에포크는 복구 동작 시 하이워터마크를 대체하는 수단으로도 활용
- 리더에포크는 컨트롤러에 의해 관리되는 32비트 숫자로 표현
- 새로운 리더로 변경된 후 변경된 리더에 대한 정보가 팔로워들에게 전달

> 브로커가 복구 동작을 하는 데 리더에포크가 필요한 이유 

##### 4.1.5.1 리더에포크를 사용하지 않은 장애 복구 과정 

- ![캡처](https://velog.velcdn.com/images/woorung/post/d0971cca-47d0-4c53-a7aa-568f9c1eb497/image.png)
- 파티션 : 1 
- 리플리케이션 팩터 : 2
- min.insync.replicas : 1 
